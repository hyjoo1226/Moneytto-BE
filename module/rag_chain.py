import os
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI, OpenAIEmbeddings # Now this import should work
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder

from langchain.prompts import ChatPromptTemplate, PromptTemplate, HumanMessagePromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_pinecone import PineconeVectorStore
from langchain_upstage import UpstageEmbeddings
from pinecone import Pinecone, ServerlessSpec

from get_docs import format_docs, load_indexing_news, get_naver_news_with_kewords


# load_dotenv()
# open_ai_key = os.environ.get("open_ai_key")

# # upstage models
# embedding_upstage = UpstageEmbeddings(model="embedding-query")

# pinecone_api_key = os.environ.get("PINECONE_API_KEY")
# pc = Pinecone(api_key=pinecone_api_key)

def get_prompt_for_type(customer_type):
    first_template = f"""
    ë‹¹ì‹ ì€ ê¸ˆìœµ ìƒë‹´ ì „ë¬¸ê°€ì´ë©°, ê³ ê°ì˜ íˆ¬ìì„±í–¥ì— ë§ëŠ” íˆ¬ììƒí’ˆì„ ì°¸ì¡°ë¬¸ì„œì— ê¸°ë°˜í•´ ì¶”ì²œí•˜ëŠ” ì—­í• ì„ ë§¡ê³  ìˆìŠµë‹ˆë‹¤.


    ê³ ê°ì˜ íˆ¬ìì„±í–¥ì€ {customer_type}í˜•ì…ë‹ˆë‹¤.
    """ + """
    1. ì°¸ì¡°ë¬¸ì„œì—ì„œ ê³ ê°ì˜ íˆ¬ìì„±í–¥ì„ ì°¾ì•„ ì¶”ì²œë˜ëŠ” íˆ¬ììƒí’ˆì„ ì°¾ìŠµë‹ˆë‹¤.
    2. ì¶”ì²œí•˜ëŠ” íˆ¬ììƒí’ˆ ì¢…ë¥˜ë¥¼ ë‚˜ì—´í•˜ê³ , ì´í›„ íˆ¬ììƒí’ˆê³¼ ê·¸ íŠ¹ì§•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.
        - ì–´ë ¤ìš´ ë‹¨ì–´ëŠ” ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ë„ë¡ í’€ì–´ì„œ ì„¤ëª…í•˜ê³  ì‰¬ìš´ ìš©ì–´ë¡œ ëŒ€ì²´í•˜ì„¸ìš”.
    3. ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  í•˜ì„¸ìš”. ëª¨ë“  ë‹µë³€ì€ ëª…í™•í•œ ê·¼ê±°ë¥¼ ê°€ì§„ ì‚¬ì‹¤ì— ê¸°ë°˜í•©ë‹ˆë‹¤. ì˜ˆ) ì§ˆë¬¸ì„ ì´í•´í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤.
    4. ì°¸ì¡°ë¬¸ì„œì™€ ì°¸ì¡°ë‰´ìŠ¤ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— í‹€ë¦° ë‚´ìš©ì´ ìˆë‹¤ë©´ ìˆ˜ì •í•´ì„œ ë‹µë³€í•˜ì„¸ìš”.
    5. ì‚¬ìš©ìì˜ ê°œì¸ì •ë³´ ë° ë¯¼ê°ì •ë³´(ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸, ì¹´ë“œì •ë³´ ë“±)ê°€ ì§ˆë¬¸ì— ë“¤ì–´ì˜¤ë©´ 'ê°œì¸ì˜ ë¯¼ê°í•œ ì •ë³´ë¥¼ ì…ë ¥í•˜ì§€ ë§ˆì„¸ìš”!'ë¼ëŠ” ê²½ê³ ë¥¼ ë„ìš°ê³  ë‹µë³€ ìƒì„±í•˜ëŠ” ê²ƒì„ ë©ˆì¶¥ë‹ˆë‹¤.
    6. ë‹µë³€ í˜•ì‹ì— ë§ì¶”ì„¸ìš”.

    
    ì§ˆë¬¸: {question}
    ì°¸ì¡°ë¬¸ì„œ: {context_pdf}
    ë‹µë³€ í˜•ì‹:
    #### ğŸ’š ë¨¸ë‹ˆë˜ë‹˜ì˜ íˆ¬ìì„±í–¥ì€ [...]ì…ë‹ˆë‹¤.
    ì¶”ì²œí•˜ëŠ” íˆ¬ììƒí’ˆì€ ... ì…ë‹ˆë‹¤.
    1. íˆ¬ììƒí’ˆ:
        - íŠ¹ì§• ë° ë¦¬ìŠ¤í¬:
    2. íˆ¬ììƒí’ˆ:
        - íŠ¹ì§• ë° ë¦¬ìŠ¤í¬:
    ...

    """
    # ì´ì „ ë©”ì‹œì§€: {chat_history}

    # ìƒˆë¡œìš´ PromptTemplate ìƒì„±
    prompt_template_after_survey = PromptTemplate(
        input_variables=["context_pdf", "question"],#, "chat_history"],  # ë³€ìˆ˜ ê·¸ëŒ€ë¡œ ìœ ì§€
        template=first_template  # ìƒˆ í…œí”Œë¦¿ ì ìš©
    )

    # HumanMessagePromptTemplate ìƒì„±
    updated_human_prompt1 = HumanMessagePromptTemplate(prompt=prompt_template_after_survey)

    # ChatPromptTemplate ê°ì²´ ìƒì„±
    updated_prompt1 = ChatPromptTemplate(messages=[updated_human_prompt1])
    
    return updated_prompt1

def get_prompt_for_recommend(choice):
    recommend_template = f"""
    ë‹¹ì‹ ì€ ê¸ˆìœµ ìƒë‹´ ì „ë¬¸ê°€ì´ë©°, ê³ ê°ì˜ ì§ˆë¬¸ì— ë§ëŠ” ëŒ€ë‹µì„ í•˜ê³  ì´ì— ëŒ€í•œ ê·¼ê±°ë¥¼ ì°¸ì¡°ë¬¸ì„œì— ê¸°ë°˜í•´ ëŒ€ë‹µí•˜ëŠ” ì—­í• ì„ ë§¡ê³  ìˆìŠµë‹ˆë‹¤.

    ê³ ê°ì´ ì›í•˜ëŠ” ê¸ˆìœµíˆ¬ììƒí’ˆì€ {choice}ì…ë‹ˆë‹¤.
    """ + """
    1. ì°¸ì¡°ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì„œ ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ê¸ˆìœµíˆ¬ììƒí’ˆê³¼ ê²½ì œ ì§€ìˆ˜ ë° ìš”ì¸ê³¼ì˜ ê´€ê³„ë¥¼ íŒŒì•…í•˜ì„¸ìš”.
        - ê¸ˆìœµíˆ¬ììƒí’ˆì´ í° ìˆ˜ìµì„±ì„ ê°€ì§€ëŠ” ì‹œì ê³¼ ë¦¬ìŠ¤í¬ë¥¼ ê°€ì§€ëŠ” ì‹œì ì„ ë¶„ì„í•˜ì„¸ìš”.
    2. 1ë²ˆ ê²°ê³¼ì™€ ì°¸ì¡°ë‰´ìŠ¤ë¥¼ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë¶€í•©í•˜ëŠ” ë‹µë³€ì„ í•˜ì„¸ìš”.
        - ìƒí’ˆ/ì¢…ëª© ì„ íƒ ê¸°ì¤€ì€ ëª…í™•í•˜ê³  ë…¼ë¦¬ì ì´ì–´ì•¼ í•˜ë©°, ì¥ë‹¨ì  ë° ë¦¬ìŠ¤í¬ê°€ ê· í˜• ìˆê²Œ í¬í•¨ë˜ë„ë¡ ì‘ì„±í•˜ì„¸ìš”.
        - ì¶”ì²œ ì´ìœ ëŠ” ë‰´ìŠ¤ë¥¼ í†µí•´ í˜„ì¬ ê²½ê¸° ìƒí™©ì„ íŒŒì•…í•˜ê³  ìš”ì•½í•œ ê²°ê³¼ë¥¼ ë³´ê³  ìµœì‹  ë™í–¥ì„ ë°˜ì˜í•©ë‹ˆë‹¤. ì´í›„ ì´ë“ì„ ì·¨í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ëŠ” ì¢…ëª©ì„ ì¶”ì²œí•˜ê³ , ê·¸ ê·¼ê±°ë¥¼ ì°¸ì¡°ë‰´ìŠ¤ì—ì„œ ì°¾ì•„ êµ¬ì²´ì ìœ¼ë¡œ ë§ë¶™ì´ì„¸ìš”.
        - ë§Œì•½, ì‚¬ìš©ìê°€ ì„ íƒí•œ ê¸ˆìœµíˆ¬ììƒí’ˆì´ ë¦¬ìŠ¤í¬ë¥¼ ê°€ì§€ëŠ” ì‹œì ì´ë¼ë©´ ì•ˆì •ì ì¸ ë‹¤ë¥¸ íˆ¬ììƒí’ˆ(ì˜ˆì ê¸ˆ, ì±„ê¶Œ, í€ë“œ ë“±ë“±)ì„ ì¶”ì²œí•©ë‹ˆë‹¤.
    3. ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  í•˜ì„¸ìš”. ëª¨ë“  ë‹µë³€ì€ ëª…í™•í•œ ê·¼ê±°ë¥¼ ê°€ì§„ ì‚¬ì‹¤ì— ê¸°ë°˜í•©ë‹ˆë‹¤. ì˜ˆ) ì§ˆë¬¸ì„ ì´í•´í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤.
    4. ì°¸ì¡°ë¬¸ì„œì™€ ì°¸ì¡°ë‰´ìŠ¤ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— í‹€ë¦° ë‚´ìš©ì´ ìˆë‹¤ë©´ ìˆ˜ì •í•´ì„œ ë‹µë³€í•˜ì„¸ìš”.
    5. ì‚¬ìš©ìì˜ ê°œì¸ì •ë³´ ë° ë¯¼ê°ì •ë³´(ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸, ì¹´ë“œì •ë³´ ë“±)ê°€ ì§ˆë¬¸ì— ë“¤ì–´ì˜¤ë©´ 'ê°œì¸ì˜ ë¯¼ê°í•œ ì •ë³´ë¥¼ ì…ë ¥í•˜ì§€ ë§ˆì„¸ìš”!'ë¼ëŠ” ê²½ê³ ë¥¼ ë„ìš°ê³  ë‹µë³€ ìƒì„±í•˜ëŠ” ê²ƒì„ ë©ˆì¶¥ë‹ˆë‹¤.
    6. ë‹µë³€ í˜•ì‹ì— ë§ì¶”ì„¸ìš”.


    
    ì§ˆë¬¸: {question}
    ì°¸ì¡°ë¬¸ì„œ: {context_pdf}
    ì°¸ì¡°ë‰´ìŠ¤: {context_news}
    ë‹µë³€ í˜•ì‹:
    - í˜„ì¬ ê²½ì œ ìƒí™©:
    - ì„ íƒí•˜ì‹  ê¸ˆìœµíˆ¬ììƒí’ˆ:
    - ì¶”ì²œ ì¢…ëª©:
    - ì¢…ëª© ì¶”ì²œ ì´ìœ :

    """
    # ì´ì „ ë©”ì‹œì§€: {chat_history}
    # ìƒˆë¡œìš´ PromptTemplate ìƒì„±
    updated_prompt_template = PromptTemplate(
        input_variables=["context_pdf", "context_news", "question"],#, "chat_history"],  # ë³€ìˆ˜ ê·¸ëŒ€ë¡œ ìœ ì§€
        template=recommend_template  # ìƒˆ í…œí”Œë¦¿ ì ìš©
    )

    # HumanMessagePromptTemplate ìƒì„±
    updated_human_prompt2 = HumanMessagePromptTemplate(prompt=updated_prompt_template)

    # ChatPromptTemplate ê°ì²´ ìƒì„±
    updated_prompt2 = ChatPromptTemplate(messages=[updated_human_prompt2])
    return updated_prompt2


def set_rag_chain_for_type(customer_type, open_ai_key, pc):#, chat_history):
    
    # upstage models
    embedding_upstage = UpstageEmbeddings(model="embedding-query")
    
    pdf_vectorstore = PineconeVectorStore(
        index=pc.Index("index-pdf"), embedding=embedding_upstage
    )
    
    pdf_retriever = pdf_vectorstore.as_retriever(
        search_type= 'mmr', # default : similarity(ìœ ì‚¬ë„) / mmr ì•Œê³ ë¦¬ì¦˜
        search_kwargs={"k": 3} # ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ chunkë¥¼ 3ê°œ ê²€ìƒ‰í•˜ê¸° (default : 4)
    )
    
    # ReRanker: CrossEncoder
    model = HuggingFaceCrossEncoder(model_name="BAAI/bge-reranker-base")
    compressor = CrossEncoderReranker(model=model, top_n=3)

    pdf_retriever_compression = ContextualCompressionRetriever(
        base_compressor=compressor, base_retriever=pdf_retriever
    )

    llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0,
                    openai_api_key=open_ai_key)

    # ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    rag_chain = (
        {"context_pdf": pdf_retriever_compression, "question": RunnablePassthrough()}#, "chat_history": chat_history} #
        | get_prompt_for_type(customer_type)
        | llm
        | StrOutputParser()
    )
    return rag_chain.invoke("ë‚˜ì˜ íˆ¬ìì„±í–¥ì— ë§ëŠ” íˆ¬ììƒí’ˆì„ ì•Œë ¤ì¤˜.")



def set_rag_chain_for_recommend(question, choice, open_ai_key, pc):#, chat_history):
    # upstage models
    embedding_upstage = UpstageEmbeddings(model="embedding-query")
    
    pdf_vectorstore = PineconeVectorStore(
        index=pc.Index("index-pdf"), embedding=embedding_upstage
    )
    
    news_vectorstore = PineconeVectorStore(
        index=pc.Index("index-news"), embedding=embedding_upstage
    )
    
    pdf_retriever = pdf_vectorstore.as_retriever(
        search_type= 'mmr', # default : similarity(ìœ ì‚¬ë„) / mmr ì•Œê³ ë¦¬ì¦˜
        search_kwargs={"k": 3} # ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ chunkë¥¼ 3ê°œ ê²€ìƒ‰í•˜ê¸° (default : 4)
    )

    # ë‰´ìŠ¤ì— í¬í•¨ë˜ì–´ ìˆëŠ” ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³  ìƒì„±í•©ë‹ˆë‹¤.
    news_retriever = news_vectorstore.as_retriever(
        search_type= 'mmr', # default : similarity(ìœ ì‚¬ë„) / mmr ì•Œê³ ë¦¬ì¦˜
        search_kwargs={"k": 3} # ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ chunkë¥¼ 3ê°œ ê²€ìƒ‰í•˜ê¸° (default : 4)
    )

    # ReRanker: CrossEncoder
    model = HuggingFaceCrossEncoder(model_name="BAAI/bge-reranker-base")
    compressor = CrossEncoderReranker(model=model, top_n=3)

    pdf_retriever_compression = ContextualCompressionRetriever(
        base_compressor=compressor, base_retriever=pdf_retriever
    )
    news_retriever_compression = ContextualCompressionRetriever(
        base_compressor=compressor, base_retriever=news_retriever
    )

    llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0,
                    openai_api_key=open_ai_key)

    # ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    rag_chain = (
        {"context_news": news_retriever_compression | format_docs, "context_pdf": pdf_retriever_compression, "question": RunnablePassthrough()}#, "chat_history": chat_history}
        | get_prompt_for_recommend(choice)
        | llm
        | StrOutputParser()
    )
    return rag_chain.invoke(question)